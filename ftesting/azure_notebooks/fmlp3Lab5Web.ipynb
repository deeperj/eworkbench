{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dropbox.com/s/l8v1b51kg1wrzib/fmlp3.PNG?dl=1\" width=300>\n",
    "# $fMLp^3$ Lab 5: Tree-based Algorithms and Support Vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction<a id=\"sec-2-1\" name=\"sec-2-1\"></a>\n",
    "\n",
    "The last two labs focused on Linear and probabilistic models. In this lab session, we consider some more interesting machine learning algorithms.  The surprisingly simple tree but versatile tree-based algorithms and the robust support vector machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression <a id=\"sec-2-2-1\" name=\"sec-2-2-1\"></a>\n",
    "\n",
    "The general equation for a straight line is given as $y=mx+c$ where m is the coefficient of the independent variable x and c is the intercept on the y-axis.\n",
    "\n",
    "This is a very simple case of a linear model having just one independent variable.  In more complex tasks we may have more than one variable which may have interdependencies amongst them. Given the dataset below of housing prices, having 2 independent variables, we develop the following linear equation.\n",
    "\n",
    "$$h_\\theta(x)=\\theta_0+\\theta_1 x_1+\\theta_2 x_2$$\n",
    "\n",
    "Again,While $\\theta_0$ is the constant, $\\theta_k (k>0)$ are the coefficients of the independet variables denoted as $x_k (k>0)$.\n",
    "\n",
    "If we introduce $x_0=1$, we can therefore use the vector inner products of $\\theta_k$ and $x_k$ as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data<a id=\"sec-2-2-2\" name=\"sec-2-2-2\"></a>\n",
    "\n",
    "1.  List the data you ned and how much you need\n",
    "2.  Find and document where you can get that data\n",
    "3.  check how much space it will take\n",
    "4.  Check legal obligations and get authorisation if necessary\n",
    "5.  Get access authorisations\n",
    "6.  Create a workspace with enough storage\n",
    "7.  get the data\n",
    "8.  Convert the data to the format you can easily manipulate without chainging the data itself\n",
    "9.  Ensure sensitive invormation is removed or protected\n",
    "10. Sample a test set, and never look at it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data<a id=\"sec-2-2-3\" name=\"sec-2-2-3\"></a>\n",
    "\n",
    "1.  Create a copy of the data for exploration (sampling it down to a manageable size if necesssary)\n",
    "2.  Create a Jupyter notebook to keep record of your data exploration\n",
    "3.  Study each attribute and its characteristics ie.e\n",
    "    -   name\n",
    "    -   type (categorical/int/float/bounded/unbounded/text/structured etc\n",
    "    -   any missing values\n",
    "    -   Noisiness and type of noise (stochastic, outlier, rounding errors etc)\n",
    "    -   Type of distribution (gaussian, uniform, log, etc)\n",
    "4.  For supervised learning, identify target attributes (features)\n",
    "5.  Visualise the data\n",
    "6.  Study the correlations between attributes\n",
    "7.  Study how you would solve the problem manually\n",
    "8.  Identify the promising transformations you may want to apply\n",
    "9.  Identify extra data that would be useful\n",
    "10. Document what you have learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data<a id=\"sec-2-2-4\" name=\"sec-2-2-4\"></a>\n",
    "\n",
    "1.  Work on copies of the data (keep originals intact)\n",
    "2.  Write functions for all data transformations you apply for 5 reasons \n",
    "    1.  So you can easily prepare the data\n",
    "    2.  So you can apply these transformations in similar situations in the future\n",
    "    3.  to clean and prepare the test set\n",
    "    4.  to clean and prepare instances once your solution is live\n",
    "    5.  to make it easy to treat your preparation choices as hyperparameters\n",
    "3.  Data Cleaning\n",
    "    -   Fix or remove outliers if need be.\n",
    "    -   Fill in missing values (with zero, mean, median) or drop rows or columns\n",
    "4.  Feature selection (optional)\n",
    "    -   Drop attributes that prodie no useful information for the task\n",
    "5.  Feature engineering where appropriate e.g.\n",
    "    -   Descretise continuous features\n",
    "    -   Decompose features (e.g. categorical, date/time, etc.)\n",
    "    -   Add promising transformations of features (e.g., log(x), sqrt(x), x<sup>2</sup>, etc)\n",
    "    -   Aggregate features into promising new features\n",
    "6.  Feature Scaling\n",
    "    -   Standardise or normalise features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short-list promising models<a id=\"sec-2-2-5\" name=\"sec-2-2-5\"></a>\n",
    "\n",
    "If the data is large, you may want to sample smaller training sets so you can train many different models in a reasonable time (be aware that this penalises complex models such as large neural nets or random forests).  Once again try to automate these steps as much as possible.\n",
    "1.  Train many quick and dirty models from different categories (e.g. linear, naive bayes, SVM, random forests, neural nets etc.) using standard parameters.\n",
    "2.  Measure and compare their performance:  For each model, use N-fold cross-validation and compute the mean and standard deviation of the performance on the N-folds.\n",
    "3.  Analyse the most significant variables for each algorithm.\n",
    "4.  Analyse the types of errors the models make and proffer how such errors can be avoided.\n",
    "5.  Have a quick round of feature selection and engineering\n",
    "6.  Have one or two more quick iterations of steps 1 to 5\n",
    "7.  Short list the top three to five most promising models, preferring models that make different types of errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune the system<a id=\"sec-2-2-6\" name=\"sec-2-2-6\"></a>\n",
    "\n",
    "1.  You will want to use as much data as possible for this step, especially as you move toward the end of fine-tuning.\n",
    "2.  Automate what you can\n",
    "3.  Fine-tune hyper parameters using cross-validation\n",
    "4.  Treat data transformation choices you are sure about as hyper parameters\n",
    "5.  Unless there are very few hyper parameter values to explore, prefer random search over grid search.  If training is very long, you may prefer a Bayesian optimisation approach using Gaussian process priors  <https://goo.gl/PEFfGr> [@snoek2012practical]\n",
    "6.  Try Ensemble methods.  Combining your best models will often perform better than running them individually.\n",
    "7.  Once you are confident about the final model, measre its performance on the test set to estimate the generalisation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Present your solution<a id=\"sec-2-2-7\" name=\"sec-2-2-7\"></a>\n",
    "\n",
    "1.  Document what you have done.\n",
    "2.  Create a presentation highlighting the big picture\n",
    "3.  Explain why your solution achieves the business objective\n",
    "4.  Present interesting points  you learned along the way.  Describe what worked and what did not. List the assumptions and system limitations.\n",
    "5.  Use visualisation to communicate key findings. e.g. the median income is the number one predictor of housing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch<a id=\"sec-2-2-8\" name=\"sec-2-2-8\"></a>\n",
    "\n",
    "1.  Plug in production data inputs, write unit tests etc.\n",
    "2.  Write monitoring code to check you system's live performance at regular intervals and trigger alerts when it drops.\n",
    "    -   Beware of slow degration as models tend to wrote as data eveloves\n",
    "    -   Performance measurement may require crowd sourcing.\n",
    "    -   Monitor inputs quality.\n",
    "3.  Retrain your models at regular basis on fresh data (automate as much as possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees<a id=\"sec-2-3\" name=\"sec-2-3\"></a>\n",
    "\n",
    "Decision trees are capable of performing both regression and classification.  They also are fundamental components of random forest.  We first look at the classification properties of decision trees before considering the regression capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree classifier\n",
    "Suppose you find an iris flower and you want to classify it.  You start at the root node (depth 0, at the top): this node asks whether the flower's petal length is smaller than 2.45cm.  If it is then you move down to the root's left child node (depth 1, left).  In this case, it is a leaf node (i.e. it does not have any children nodes), so it does not ask any more questions: you can simply look at the predicted class for that node and the decision tree predicts that the flower belongs to the class (Class=Setosa).\n",
    "\n",
    "Now suppose you find another flow, but this time the petal length is greater than 2.45 cm.  You must move down to the root's right child node (depth 2, left).  If not, it is likely an Iris-Virginica (depth 2, right).\n",
    "\n",
    "One nice quality of decision trees is that they require much less data preparation, in particular decision trees are neutral to normalisation of the data.\n",
    "\n",
    "A node's samples attribute counts how many training instances it applies to.  for example, 100 training instances have a petal length greater than 2.45cm (depth 1, right), among  which 54 have a petal width smaller than 1.75cm (depth 2 left).  A node's value attribute tells you how many training instances of each class this node applies to: for example, the bottom-right node applies to 0 Iris-Setosa, 1 Iris-Versicolor and 45 Iris-Virginica.  Finally, a node's gini attribute measures its impurity: a node is \"pure\"(gini=0) if all training instances it applies to belong to the same class.  For example, since the depth-1 left node applies to only Iris-Setosa training instances, it is pure and its gini score is 0.  Equation (1) below shows how the training algorithm computes the gini Score $G_i$ of the ith node.  For example, the depth 2 left node has a gini score equal to $1-(0/54)^2-(49/54)^2-(5/54)\\approx 0.168$.  Another impurity measure will be disccussed shortly.\n",
    "$$G_i=1-\\sum_{k=1}^np_{i,k}^2 - - (5.1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p_{i,k}$ is the ratio of class k instances among the training instances in the ith node.\n",
    "\n",
    "Figure (6.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           0           5.1          3.5           1.4          0.2  setosa\n",
       "1           1           4.9          3.0           1.4          0.2  setosa\n",
       "2           2           4.7          3.2           1.3          0.2  setosa\n",
       "3           3           4.6          3.1           1.5          0.2  setosa\n",
       "4           4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "iris=pd.read_csv('iris.csv')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setosa        50\n",
      "versicolor    50\n",
      "virginica     50\n",
      "Name: species, dtype: int64\n",
      "<class 'pandas.core.series.Series'>\n",
      "50\n",
      "[50 50 50]\n",
      "Index(['setosa', 'versicolor', 'virginica'], dtype='object')\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "chk=iris.species.value_counts()\n",
    "print(chk)\n",
    "print(type(chk))\n",
    "print(chk.versicolor)\n",
    "print(chk.values)\n",
    "print(chk.index)\n",
    "print(type(64.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels=iris['species'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=iris['petal_width'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>lab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data     lab\n",
       "0   0.2  setosa\n",
       "1   0.2  setosa\n",
       "2   0.2  setosa\n",
       "3   0.2  setosa\n",
       "4   0.2  setosa"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=pd.DataFrame({'data':data, 'lab':labels})\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the probability of each label will be the value_counts divided by the sum of the value counts.  We can perform this in a single line of code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "versicolor    0.333333\n",
       "setosa        0.333333\n",
       "virginica     0.333333\n",
       "Name: species, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chk/sum(chk.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, to obtain the mean and variance for the combined dataset is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>lab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.4</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.5</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.5</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.3</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1.5</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    data         lab\n",
       "50   1.4  versicolor\n",
       "51   1.5  versicolor\n",
       "52   1.5  versicolor\n",
       "53   1.3  versicolor\n",
       "54   1.5  versicolor"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = [dataset.loc[dataset.lab==label] for label in chk.index]\n",
    "ds[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "versicolor    50\n",
       "setosa        50\n",
       "virginica     50\n",
       "Name: lab, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.lab.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3259999999999998, 0.2459999999999999, 2.026]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means=[d.data.mean() for d in ds]\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.039106122448979576, 0.0111061224489796, 0.07543265306122447]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvars=[d.data.var() for d in ds]\n",
    "dvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete Implementation is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Appendix 1: Word2Vec Co-occurrence Matrix Decomposition with SVD Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For the co-occurrence model we count the number of times a certain number of words occur together take for example a vocuabulary of words and a count of the times two words co-occur together in the illustration below:\n",
    "![Co-occurrence Matrix](https://www.dropbox.com/s/pcl4c8lf5as56tl/svd1.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Corpus:\n",
    "- I like deep learning.\n",
    "- I like NLP.\n",
    "- I enjoy flying.\n",
    "\n",
    "The word vectors can therefore be determined using Singular Value Decomposition (SVD) as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAFqCAYAAAA5ngEFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEnNM7BIkqTm/X+TwPVu5jdttAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe30f6c89b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "la=np.linalg\n",
    "words=['I','like','enjoy','deep','learning','NLP','flying','.']\n",
    "x=np.array([[0,2,1,0,0,0,0,0],\n",
    "            [2,0,0,1,0,1,0,0],\n",
    "            [1,0,0,0,0,0,1,0],\n",
    "            [0,1,0,0,1,0,0,0],\n",
    "            [0,0,0,1,0,0,0,1],\n",
    "            [0,1,0,0,0,0,0,1],\n",
    "            [0,0,1,0,0,0,0,1],\n",
    "            [0,0,0,0,1,1,1,0]\n",
    "           ])\n",
    "U,s,Vh=la.svd(x,full_matrices=False)\n",
    "for i in range(len(words)):\n",
    "    plt.text(U[i,0],U[i,1],words[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -5.24124930e-01,  -5.72859145e-01,   9.54463014e-02,\n",
       "          3.83228493e-01,  -1.76963375e-01,  -1.76092183e-01,\n",
       "         -4.19185600e-01,  -5.57702732e-02],\n",
       "       [ -5.94438071e-01,   6.30120664e-01,  -1.70207779e-01,\n",
       "          3.10038363e-01,   1.84062339e-01,  -2.34777849e-01,\n",
       "          1.29535474e-01,   1.36813128e-01],\n",
       "       [ -2.56274005e-01,   2.74017533e-01,   1.59810848e-01,\n",
       "         -5.55111512e-16,  -5.78984617e-01,   6.36550929e-01,\n",
       "         -6.10622664e-16,  -3.05414877e-01],\n",
       "       [ -2.85637408e-01,  -2.47912130e-01,   3.54610324e-01,\n",
       "         -7.31901294e-02,   4.45784489e-01,   8.36141432e-02,\n",
       "          5.48721075e-01,  -4.68012411e-01],\n",
       "       [ -1.93139313e-01,   3.38495048e-02,  -5.00790405e-01,\n",
       "         -4.28462480e-01,   3.47110226e-01,   1.55483227e-01,\n",
       "         -4.68663749e-01,  -4.03576557e-01],\n",
       "       [ -3.05134684e-01,  -2.93988990e-01,  -2.23433593e-01,\n",
       "         -1.91614246e-01,   1.27460940e-01,   4.91219408e-01,\n",
       "          2.09592800e-01,   6.57535375e-01],\n",
       "       [ -1.82489837e-01,  -1.61027767e-01,  -3.97842428e-01,\n",
       "         -3.83228493e-01,  -5.12923221e-01,  -4.27574417e-01,\n",
       "          4.19185600e-01,  -1.18313828e-01],\n",
       "       [ -2.46898426e-01,   1.57254762e-01,   5.92991677e-01,\n",
       "         -6.20076727e-01,  -3.21868120e-02,  -2.31065080e-01,\n",
       "         -2.59070949e-01,   2.37976916e-01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
