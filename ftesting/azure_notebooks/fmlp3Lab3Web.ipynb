{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dropbox.com/s/l8v1b51kg1wrzib/fmlp3.PNG?dl=1\" width=300>\n",
    "# $fMLp^3$ Lab 3: Generalised Linear Model and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction<a id=\"sec-2-1\" name=\"sec-2-1\"></a>\n",
    "\n",
    "In previous labs we have applied logistic regression to various classification tasks using the of the shelf models within the scikit-learn machine learning libray.  In today's class we take a look under the hood by creating our own models. Linear and logistic regression follow similar modelling procedures and therefore fall into the class of machine learning models known as generalised linear models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression <a id=\"sec-2-2-1\" name=\"sec-2-2-1\"></a>\n",
    "\n",
    "The general equation for a straight line is given as $y=mx+c$ where m is the coefficient of the independent variable x and c is the intercept on the y-axis.\n",
    "\n",
    "This is a very simple case of a linear model having just one independent variable.  In more complex tasks we may have more than one variable which may have interdependencies amongst them. Given the dataset below of housing prices, having 2 independent variables, we develop the following linear equation.\n",
    "\n",
    "$$h_\\theta(x)=\\theta_0+\\theta_1 x_1+\\theta_2 x_2$$\n",
    "\n",
    "Again,While $\\theta_0$ is the constant, $\\theta_k (k>0)$ are the coefficients of the independet variables denoted as $x_k (k>0)$.\n",
    "\n",
    "If we introduce $x_0=1$, we can therefore use the vector inner products of $\\theta_k$ and $x_k$ as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data<a id=\"sec-2-2-2\" name=\"sec-2-2-2\"></a>\n",
    "\n",
    "1.  List the data you ned and how much you need\n",
    "2.  Find and document where you can get that data\n",
    "3.  check how much space it will take\n",
    "4.  Check legal obligations and get authorisation if necessary\n",
    "5.  Get access authorisations\n",
    "6.  Create a workspace with enough storage\n",
    "7.  get the data\n",
    "8.  Convert the data to the format you can easily manipulate without chainging the data itself\n",
    "9.  Ensure sensitive invormation is removed or protected\n",
    "10. Sample a test set, and never look at it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data<a id=\"sec-2-2-3\" name=\"sec-2-2-3\"></a>\n",
    "\n",
    "1.  Create a copy of the data for exploration (sampling it down to a manageable size if necesssary)\n",
    "2.  Create a Jupyter notebook to keep record of your data exploration\n",
    "3.  Study each attribute and its characteristics ie.e\n",
    "    -   name\n",
    "    -   type (categorical/int/float/bounded/unbounded/text/structured etc\n",
    "    -   any missing values\n",
    "    -   Noisiness and type of noise (stochastic, outlier, rounding errors etc)\n",
    "    -   Type of distribution (gaussian, uniform, log, etc)\n",
    "4.  For supervised learning, identify target attributes (features)\n",
    "5.  Visualise the data\n",
    "6.  Study the correlations between attributes\n",
    "7.  Study how you would solve the problem manually\n",
    "8.  Identify the promising transformations you may want to apply\n",
    "9.  Identify extra data that would be useful\n",
    "10. Document what you have learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data<a id=\"sec-2-2-4\" name=\"sec-2-2-4\"></a>\n",
    "\n",
    "1.  Work on copies of the data (keep originals intact)\n",
    "2.  Write functions for all data transformations you apply for 5 reasons \n",
    "    1.  So you can easily prepare the data\n",
    "    2.  So you can apply these transformations in similar situations in the future\n",
    "    3.  to clean and prepare the test set\n",
    "    4.  to clean and prepare instances once your solution is live\n",
    "    5.  to make it easy to treat your preparation choices as hyperparameters\n",
    "3.  Data Cleaning\n",
    "    -   Fix or remove outliers if need be.\n",
    "    -   Fill in missing values (with zero, mean, median) or drop rows or columns\n",
    "4.  Feature selection (optional)\n",
    "    -   Drop attributes that prodie no useful information for the task\n",
    "5.  Feature engineering where appropriate e.g.\n",
    "    -   Descretise continuous features\n",
    "    -   Decompose features (e.g. categorical, date/time, etc.)\n",
    "    -   Add promising transformations of features (e.g., log(x), sqrt(x), x<sup>2</sup>, etc)\n",
    "    -   Aggregate features into promising new features\n",
    "6.  Feature Scaling\n",
    "    -   Standardise or normalise features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short-list promising models<a id=\"sec-2-2-5\" name=\"sec-2-2-5\"></a>\n",
    "\n",
    "If the data is large, you may want to sample smaller training sets so you can train many different models in a reasonable time (be aware that this penalises complex models such as large neural nets or random forests).  Once again try to automate these steps as much as possible.\n",
    "1.  Train many quick and dirty models from different categories (e.g. linear, naive bayes, SVM, random forests, neural nets etc.) using standard parameters.\n",
    "2.  Measure and compare their performance:  For each model, use N-fold cross-validation and compute the mean and standard deviation of the performance on the N-folds.\n",
    "3.  Analyse the most significant variables for each algorithm.\n",
    "4.  Analyse the types of errors the models make and proffer how such errors can be avoided.\n",
    "5.  Have a quick round of feature selection and engineering\n",
    "6.  Have one or two more quick iterations of steps 1 to 5\n",
    "7.  Short list the top three to five most promising models, preferring models that make different types of errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune the system<a id=\"sec-2-2-6\" name=\"sec-2-2-6\"></a>\n",
    "\n",
    "1.  You will want to use as much data as possible for this step, especially as you move toward the end of fine-tuning.\n",
    "2.  Automate what you can\n",
    "3.  Fine-tune hyper parameters using cross-validation\n",
    "4.  Treat data transformation choices you are sure about as hyper parameters\n",
    "5.  Unless there are very few hyper parameter values to explore, prefer random search over grid search.  If training is very long, you may prefer a Bayesian optimisation approach using Gaussian process priors  <https://goo.gl/PEFfGr> [@snoek2012practical]\n",
    "6.  Try Ensemble methods.  Combining your best models will often perform better than running them individually.\n",
    "7.  Once you are confident about the final model, measre its performance on the test set to estimate the generalisation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Present your solution<a id=\"sec-2-2-7\" name=\"sec-2-2-7\"></a>\n",
    "\n",
    "1.  Document what you have done.\n",
    "2.  Create a presentation highlighting the big picture\n",
    "3.  Explain why your solution achieves the business objective\n",
    "4.  Present interesting points  you learned along the way.  Describe what worked and what did not. List the assumptions and system limitations.\n",
    "5.  Use visualisation to communicate key findings. e.g. the median income is the number one predictor of housing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch<a id=\"sec-2-2-8\" name=\"sec-2-2-8\"></a>\n",
    "\n",
    "1.  Plug in production data inputs, write unit tests etc.\n",
    "2.  Write monitoring code to check you system's live performance at regular intervals and trigger alerts when it drops.\n",
    "    -   Beware of slow degration as models tend to wrote as data eveloves\n",
    "    -   Performance measurement may require crowd sourcing.\n",
    "    -   Monitor inputs quality.\n",
    "3.  Retrain your models at regular basis on fresh data (automate as much as possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction vs Data Cleaning<a id=\"sec-2-3\" name=\"sec-2-3\"></a>\n",
    "\n",
    "Feature extraction and data cleaning could almost be used interchangeably, however, there is a fine difference between the two.  While data cleaning is a procedural concept, feature engineering requires skills acquistion by experience and experimentation. In other words, data cleaning operations are mostly bye-products of the feature engineering process. These feature engineering tips will be highlighted as we walk through the data cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering Features from the Iris Database using Scikit-learn Machine Learning Library\n",
    "In the last laboratory session, we studied two foundational python libraries namely, the NumPy and Pandas libraries.  In today's session we will look at the next library we shall be referencing throughout this course.  This is the scikit-learn library.  This is the major library in python used for data analytics and machine learning. \n",
    "\n",
    "We saw from the last laboratory session that the Iris dataset is a collection of data from three types of flowers of the Iris generic species i.e. the versicolor, virginica and setosa.  We were able to use the LDA algorithm and one feature (i.e. the petal width) to perform some form of classification.  Also based on information from plots of the dataset we are not sure if the petal width is the best feature used for discrimination.  In today's session, we will implement the last session's challenge and at the same time perform data cleaning and feature engineering.  We start by importing all the necessary libraries and with the class definition we started upon in the last lab session.  The first thing will be to implement a single variable LDA and in the second case we will implement the multivariable case and finally the generic case that can morph into whatever instance is required which in OOP is referred to as object polymorphism or the inheritance case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Variable LDA Class Implementation\n",
    "\n",
    "In this example we shall be adopting the Scikit-learn convention for implementing machine learning algorithms through their machine learning pipeline.  In this paradigm, we accept the process of getting results from the data will include an overall 2-step process\n",
    "1. Train the data:  This is actually more tha two steps because we assume that the data has been cleaned and feature engineered.  However once this is done, the training process creates the model we will be using to get the outputs.\n",
    "2. Fitting the data:  Once the model is ready, we can then use the model to perform data analysis which would either be a classification or regression analysis.\n",
    "\n",
    "Just as we had pre-model processing steps, we also have post-analysis tasks that we need to consider as part of the Machine Learning Pipeline or ML-Cycle if you like.  These steps include tasks like results and model evaluation and going live.  None the less the core steps in machine learning involve these two steps of Training and fitting data.\n",
    "\n",
    "Thus the Scikit-learn convention of Data analysis includes classes that perform two core operations, training which is given the method Model.fit() and fitting new data to the model i.e. Model.classify(). You need to forgive them for the slight mix up in the terminology but it does follow logically if you think about it deeply.  That is both in model training and model application we are trying to perform one type of fit of one thing against the other.  In training we are fitting the model to the data, while in the model application stage we are trying to see where the data fits given the model.  So to remove that ambiguity they decided to give it the name classify so it doesn't conflict with the previous fit operation.  I guess they may have decided to call the two operations Model.train() and Model.apply() but there you go.  But let us get back on track.  Based on this convention the skeleton of our LDA machine learning class should look like this:\n",
    "```python\n",
    "class DTClass:# Data Preparation Family\n",
    "    def __init__(): #initialise class constructor\n",
    "    def fit():\n",
    "    def Transform():\n",
    "```\n",
    "```python\n",
    "class MLClass:# Machine Learning Family\n",
    "    def __init__(): #initialise class constructor\n",
    "    def fit():\n",
    "    def predict():#we called classify()\n",
    "```\n",
    "This gives us a better understanding of the initial implementation we did here below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class LDA:\n",
    "    def __init__(self, data=[],labels=[]):\n",
    "        self.data=data\n",
    "        self.labels=labels\n",
    "        self.fit()\n",
    "        \n",
    "    def fit(self, data=[], labels=[]):\n",
    "        if len(data)>0:\n",
    "            self.data=data\n",
    "        if len(label)>0:\n",
    "            self.labels=labels\n",
    "        if len(self.data)==0 or len(self.labels)==0 or len(self.labels)!=len(self.data):\n",
    "            return print(\"No Data Available or Data mismatch!\")\n",
    "        \n",
    "    def classify(self, x):#predict\n",
    "        #self.fit()\n",
    "        self.true_labels\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is implemented such that the X and Y data can be added either in the instance definition or when you attempt to fit the data.  When you attempt to fit the data it will check that there is actually data and that the Labels and the data have the same number of instances.  Now let us add the codes we tested in the last implementation into the fit and classify method as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another point to observe is we are importing the iris dataset from a csv file which will be typical with a lot of real world practical applications we would be implementing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class irisLDA:\n",
    "    def __init__(self):\n",
    "        self.iris=pd.read_csv('iris.csv')\n",
    "        self.fit()\n",
    "        \n",
    "    def fit(self):\n",
    "        dset=self.iris.loc[self.iris.species=='setosa']\n",
    "        dvirg=self.iris.loc[self.iris.species=='virginica']\n",
    "        dvers=self.iris.loc[self.iris.species=='versicolor']\n",
    "        \n",
    "\n",
    "        self.p_virg=self.p_vers=self.p_set=1/3\n",
    "\n",
    "        self.set_m=dset.petal_width.mean()\n",
    "        self.vers_m=dvers.petal_width.mean()\n",
    "        self.virg_m=dset.petal_width.mean()\n",
    "\n",
    "        self.set_v=dset.petal_width.var()\n",
    "        self.vers_v=dvers.petal_width.var()\n",
    "        self.virg_v=dvirg.petal_width.var()\n",
    "\n",
    "    def dx(self,mk,vk,pk,xx):\n",
    "        return xx*mk/vk-mk**2/(2*vk)+np.log(pk)\n",
    "\n",
    "    def classify(self, x):\n",
    "        #self.fit()\n",
    "        res=self.dx(np.array([self.set_m,self.virg_m,self.vers_m]),\\\n",
    "                    np.array([self.set_v,self.virg_v,self.vers_v]),\\\n",
    "                    np.array([self.p_set,self.p_virg,self.p_vers]),x)\n",
    "        return 'setosa' if np.argmax(res)==0 else 'virginica' if np.argmax(res)==1 else 'versicolor'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternate Classify Method\n",
    "```python\n",
    "   def classify(self, x):\n",
    "        res1=self.dx(self.set_m,self.set_v,self.p_set,x) #these are\n",
    "        # parameters for setosa\n",
    "        res2=self.dx(self.virg_m,self.virg_v,self.p_virg,x) # these are\n",
    "        # parameters for virginica\n",
    "        res3=self.dx(self.vers_m,self.vers_v,self.p_vers,x) # these are \n",
    "        # parameters for versicolor\n",
    "        \n",
    "        if np.argmax([res1,res2,res3])==0: #since res1 is the first in \n",
    "        # array i.e. index 0 is discriminant for setosa\n",
    "            return 'setosa'\n",
    "        elif np.argmax([res1,res2,res3])==1: # 1 is index for virginica \n",
    "        # discriminant\n",
    "            return 'virginica'\n",
    "        else\n",
    "            return 'versicolor'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 5 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "L=[4,5,6] \n",
    "print(L[0],L[1],L[2])\n",
    "np.argmax(L) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'versicolor'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisLDA().classify(2.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setosa'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisLDA().classify(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the class works but doesn't feel robust enough because it can only work for iris dataset which is fine but we can do better.  In this class we see here the discriminating function dx appearing as a operation in the class that is called by the classify method.  This class though restrictive shows how we can arrange our code based on the fit and classify method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning, Feature Engineering and Generic LDA\n",
    "\n",
    "It is time to get into the nitty gritty of data cleaning and feature extraction.  In the single variable case we just chose a random-feature to implement our LDA algorithm. This is not usually the best practice for Machine Learning.  The ideal thing to do is to study the data and using a combination of testing and best practices select the most promising features.  This would probably involve visualising the data and performing various data transformations we will introduce in this sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing a Generic Single Variable LDA\n",
    "\n",
    "For this single variabel Generic LDA implementation, perform the following steps:\n",
    "\n",
    "Input data extraction:\n",
    "1. Extract labels columns (target or y-values) from the dataset\n",
    "2. Extract the chosen feature from the dataset (training or x-values)\n",
    "3. Feed them into the Single Variable LDA class either from the initialiser or the fit function.\n",
    "4. Call the classify  method\n",
    "\n",
    "Within the Single Variable Generic LDA (SVG-LDA) class we perform the following:\n",
    "1. Perform data integrity checks this would include\n",
    "   - lengths check\n",
    "   - Number of columns check\n",
    "2. Extract the number of classes from the labelled data\n",
    "3. Implement remainder of algorithm previously implemented in the irisLDA class namely:\n",
    "   - Extract sub data set containing each of the labels\n",
    "   - Calculate the discriminating parameters of each of the labels or classes\n",
    "   - The discriminating and classification functions remain the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see based on the above process that the fit function and constructor only will be modified from the irisLDA class and the discrimanatory and classify functions remains unmodified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For step 2 to extract classes from labeled data we use the pandas library. In the sample code below, we first use the read_csv() function to open the file then, we call the head() function to determine the labels column.  Next, we call the value_counts() functions to show the individual labels and their counts.  This function returns a pandas series or 1-D indexed array.  We can use the index and values property to get the array of indexes which are the string labels or the values array which are the counts.  We need both of these properties for the step 2 and 3 of the SVG-LDA implementation.  We should also put a check to ensure that the number of classes or labels are greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           0           5.1          3.5           1.4          0.2  setosa\n",
       "1           1           4.9          3.0           1.4          0.2  setosa\n",
       "2           2           4.7          3.2           1.3          0.2  setosa\n",
       "3           3           4.6          3.1           1.5          0.2  setosa\n",
       "4           4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "iris=pd.read_csv('iris.csv')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setosa        50\n",
      "versicolor    50\n",
      "virginica     50\n",
      "Name: species, dtype: int64\n",
      "<class 'pandas.core.series.Series'>\n",
      "50\n",
      "[50 50 50]\n",
      "Index(['setosa', 'versicolor', 'virginica'], dtype='object')\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "chk=iris.species.value_counts()\n",
    "print(chk)\n",
    "print(type(chk))\n",
    "print(chk.versicolor)\n",
    "print(chk.values)\n",
    "print(chk.index)\n",
    "print(type(64.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can compute the various parameters for the LDA algorithm we need to merge the labels and data into one dataset.  we shal use the pandas dataframe object to merge the two columns containing the petal_width and the species labels into one dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels=iris['species'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=iris['petal_width'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>lab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data     lab\n",
       "0   0.2  setosa\n",
       "1   0.2  setosa\n",
       "2   0.2  setosa\n",
       "3   0.2  setosa\n",
       "4   0.2  setosa"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=pd.DataFrame({'data':data, 'lab':labels})\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the probability of each label will be the value_counts divided by the sum of the value counts.  We can perform this in a single line of code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "versicolor    0.333333\n",
       "setosa        0.333333\n",
       "virginica     0.333333\n",
       "Name: species, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chk/sum(chk.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, to obtain the mean and variance for the combined dataset is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>lab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.4</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.5</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.5</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.3</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1.5</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    data         lab\n",
       "50   1.4  versicolor\n",
       "51   1.5  versicolor\n",
       "52   1.5  versicolor\n",
       "53   1.3  versicolor\n",
       "54   1.5  versicolor"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = [dataset.loc[dataset.lab==label] for label in chk.index]\n",
    "ds[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "versicolor    50\n",
       "setosa        50\n",
       "virginica     50\n",
       "Name: lab, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.lab.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3259999999999998, 0.2459999999999999, 2.026]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means=[d.data.mean() for d in ds]\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.039106122448979576, 0.0111061224489796, 0.07543265306122447]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvars=[d.data.var() for d in ds]\n",
    "dvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete Implementation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class SVG_LDA:\n",
    "    def __init__(self, data=[],labels=[]):\n",
    "        self.data=data\n",
    "        self.labels=labels\n",
    "        self.initialised=False\n",
    "        self.fit()\n",
    "        \n",
    "    def fit(self, data=[],labels=[]):\n",
    "        if len(data)>0:\n",
    "            self.data=data\n",
    "        if len(label)>0:\n",
    "            self.labels=labels\n",
    "        if len(self.data)==0 or len(self.labels)==0 or len(self.labels)!=len(self.data):\n",
    "            return print(\"No Data Available or Data mismatch!\")\n",
    "\n",
    "        self.unique_labels=labels.value_counts()\n",
    "        \n",
    "        self.probs=self.unique_labels/sum(self.unique_label.values).values\n",
    "        \n",
    "        self.unique_labels=self.unique_labels.index\n",
    "        \n",
    "        #check labels are greater than one\n",
    "        if (len(self.unique_labels)<2):\n",
    "            return print(\"Insufficient data in dataset\")\n",
    "\n",
    "        self.ds = [dataset.loc[dataset.lab==label] for label in self.unique_labels]\n",
    "\n",
    "        self.means=[d.data.mean() for d in self.ds]\n",
    "\n",
    "        self.vars=[d.data.var() for d in self.ds]\n",
    "        \n",
    "        self.initialised=True\n",
    "\n",
    "    def dx(self,mk,vk,pk,xx):\n",
    "        return xx*mk/vk-mk**2/(2*vk)+np.log(pk)\n",
    "\n",
    "    def classify(self, x):# predict\n",
    "        if(not self.initialised):\n",
    "            return print(\"Dataset is not valid!\")\n",
    "        res=self.dx(self.means,self.vars,self.probs,x)\n",
    "        return self.unique_labels[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code implements the SVG_LDA algorithm.  Notice again the changes within the fit and classify methods, while there are no changes within the dx method.  The changes are to cater for the fact that we do not know ahead of time what the labels are therefore we must put the labels in an array at run time and process the data accordingly by looping through the labels array.  Observe that this process does simplify the classify method because numpy is able to handle any math operation on an element wise basis.  This same numpy trick ensures that the dx method also remain unmodified.\n",
    "\n",
    "Robust as this code is it is not yet bullet proof in our next iteration we want to implement a Generic LDA algorithm that can handle both the single variable and multi-variable case by checking if the incoming data has more than one column.  If it does have more than one column then it is a multivariable combination if not a single variable combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Generic LDA Implementation (inc)\n",
    "\n",
    "Recall the formula for the Multivariable LDA as follows:\n",
    "$$D_k(x)=x\\times\\frac{\\sum_{i=1}^m\\frac{\\mu_k}{m}}{\\Delta\\Sigma^2_k}-\\frac{\\sum_{i=1}^m\\frac{\\mu_k^2}{m}}{2\\times \\Delta\\Sigma^2_k}+\\ln(P(k)) - - - (1)$$\n",
    "where\n",
    "$$\\begin{matrix}\n",
    " D_k(x)&=&\\text{Classification of data, x}& \\\\\n",
    " k&=&\\text{class k}& \\\\\n",
    " \\mu_k &=& \\frac{1}{n_k}\\sum_{i=1}^nx_i\n",
    "& \\text{mean of class k} - - - (2)\\end{matrix}$$\n",
    "$$\\begin{matrix}\n",
    " \\Delta\\Sigma^2_k &=& det\\left[\\frac{1}{n-K}\\sum_{i=1}^n(x_i-\\mu_{ik})(x_j-\\mu_{jk})\\right] & \\text{determinant of covariance matrix of class k}\n",
    "\\end{matrix} - - - (3)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the multi-variable LDA implementation, we need to cater for multiple independent variable.  The formula above indicates that we will require the average means of all the columns involved in the input data as well as the covariance matrix determinant values.  We therefore need to perform the following tasks:\n",
    "\n",
    "1. Obtain the number of columns in a dataset.\n",
    "2. Obtain the covariance matrix of a pandas dataframe\n",
    "3. Obtain the determinant of a matrix in pandas/numpy\n",
    "4. Means to switch between the single-variable and multi-variable implementation  to obtain a full generic LDA implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's obtain a new dataset comprising all the columns minus the 'species' label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width\n",
       "0           5.1          3.5           1.4          0.2\n",
       "1           4.9          3.0           1.4          0.2\n",
       "2           4.7          3.2           1.3          0.2\n",
       "3           4.6          3.1           1.5          0.2\n",
       "4           5.0          3.6           1.4          0.2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=iris.drop('species',axis=1)\n",
    "#we also drop unnamed:0 as it was added during import\n",
    "dataset=dataset.drop('Unnamed: 0',axis=1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 1 get the number of columns in the dataset\n",
    "dataset.columns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sepal_length</th>\n",
       "      <td>0.685694</td>\n",
       "      <td>-0.042434</td>\n",
       "      <td>1.274315</td>\n",
       "      <td>0.516271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sepal_width</th>\n",
       "      <td>-0.042434</td>\n",
       "      <td>0.189979</td>\n",
       "      <td>-0.329656</td>\n",
       "      <td>-0.121639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petal_length</th>\n",
       "      <td>1.274315</td>\n",
       "      <td>-0.329656</td>\n",
       "      <td>3.116278</td>\n",
       "      <td>1.295609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petal_width</th>\n",
       "      <td>0.516271</td>\n",
       "      <td>-0.121639</td>\n",
       "      <td>1.295609</td>\n",
       "      <td>0.581006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sepal_length  sepal_width  petal_length  petal_width\n",
       "sepal_length      0.685694    -0.042434      1.274315     0.516271\n",
       "sepal_width      -0.042434     0.189979     -0.329656    -0.121639\n",
       "petal_length      1.274315    -0.329656      3.116278     1.295609\n",
       "petal_width       0.516271    -0.121639      1.295609     0.581006"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 2: Calculate the covariance matrix\n",
    "dataset.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0019127296684331877"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 3: Calculate the determinant of the covariance matrix\n",
    "np.linalg.det(dataset.cov())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being equiped with all the prequisites for FG_LDA the implementation becomes a trivial task as follows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class FG_LDA:\n",
    "    def __init__(self, data=[],labels=[]):\n",
    "        self.data=data\n",
    "        self.labels=labels\n",
    "        self.initialised=False\n",
    "        self.fit()\n",
    "        \n",
    "    def fit(self, data=[],labels=[]):\n",
    "        if len(data)>0:\n",
    "            self.data=data\n",
    "        if len(label)>0:\n",
    "            self.labels=labels\n",
    "        if len(self.data)==0 or len(self.labels)==0 or len(self.labels)!=len(self.data):\n",
    "            return print(\"No Data Available or Data mismatch!\")\n",
    "\n",
    "        self.unique_labels=labels.value_counts()\n",
    "        \n",
    "        self.probs=self.unique_labels/sum(self.unique_label.values).values\n",
    "        \n",
    "        self.unique_labels=self.unique_labels.index\n",
    "        \n",
    "        #check labels are greater than one\n",
    "        if (len(self.unique_labels)<2):\n",
    "            return print(\"Insufficient data in dataset\")\n",
    "\n",
    "        self.ds = [dataset.loc[dataset.lab==label] for label in self.unique_labels]\n",
    "\n",
    "        self.means=[d.data.mean() for d in self.ds]\n",
    "\n",
    "        self.vars=[d.data.var() for d in self.ds]\n",
    "        \n",
    "        self.initialised=True\n",
    "\n",
    "    def dx(self,mk,vk,pk,xx):\n",
    "        return xx*mk/vk-mk**2/(2*vk)+np.log(pk)\n",
    "\n",
    "    def classify(self, x):\n",
    "        if(not self.initialised):\n",
    "            return print(\"Dataset is not valid!\")\n",
    "        res=self.dx(self.means,self.vars,self.probs,x)\n",
    "        return self.unique_labels[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Scikit-Learn: Processing Textual Information\n",
    "\n",
    "In this section will use logistic regression (classification) for the iris database to see how we can analyse textual data.\n",
    "\n",
    "The good thing about the LDA algorithm we rigorously implemented is the fact that it can handle labels as text.  Not all machine learning algorithms have the ability to do this.  For instance logistic regression maps the output to zero or one and therefore labels need to be encoded in order to perform logistic classification.\n",
    "\n",
    "As far as processing textual information as numerical data in machine learning is concerned, there are two major techniques for achieving this:\n",
    "1. Using categorical information\n",
    "2. Using one-hot encoding\n",
    "\n",
    "We discuss the two methods, however in practice it is safe to say that one-hot encoding is by far more common and natural to use even though quite counter intuitive. The one hot encoding also has a disadvantage of encouraging data sparsity which is in practice is memory expensive.  While categorical data assigns a positive integer to each feature type in the data column, one hot encoding creates extra columns for each data type in the data column and assigns a binary value of one where the feature class exists and zero on every other row.\n",
    "\n",
    "For the iris dataset for instance, categorical encocding will assign 0 to setosa, 1 to virginica and 2 to versicolor.  The sample code is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing['ocean_proximity'].value_counts()\n",
    "\n",
    "# In[26]:\n",
    "housing_cat=housing['ocean_proximity']\n",
    "housing_cat.head(10)\n",
    "\n",
    "# In[27]:\n",
    "housing_cat_encoded,housing_categories=housing_cat.factorize()\n",
    "housing_cat_encoded[:10]\n",
    "\n",
    "# In[28]:\n",
    "y=pd.Series(housing_cat_encoded)\n",
    "\n",
    "# In[29]:\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one hot encoding version is achieved by making use of the scikit-learn library as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### SkLearn One Hot Encoder\n",
    "# In[30]:\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder=OneHotEncoder()\n",
    "housing_cat_1hot=encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
    "housing_cat_1hot\n",
    "\n",
    "# In[31]:\n",
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen one method of creating a DataFrame from individual DataFrame columns.  The code below shows a method of combining existing DataFrame by extending from Scikit-Learn's Transformer class which is a generic class for Datatransformations carried.\n",
    "\n",
    "In addition to column addition, a stadard machine learning transformation which is crucial for a handful of Machine learning algorithms is the dataset normalisation operation. The code below also implements data normalisation as well as demonstrates how the various data cleaning steps can be implemented in the Scikit-Learn's pipeline.  Thus, the process of data cleaning can be implemented as an automated that can be reused and finetuned according to parameters we chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### Feature Scaling Pipeline\n",
    "# \n",
    "# In[33]:\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# column index\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "\n",
    "# In[38]:\n",
    "housing_extra_attribs = pd.DataFrame(housing_extra_attribs, columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"])\n",
    "housing_extra_attribs.head()\n",
    "\n",
    "# In[39]:\n",
    "housing.head()\n",
    "\n",
    "# In[40]:\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline=Pipeline([\n",
    "    ('imputer', Imputer(strategy=\"median\")),\n",
    "    ('attribs_adder',CombinedAttributesAdder()),\n",
    "    ('std_scaler',StandardScaler()),\n",
    "    ])\n",
    "housing_num_tr=num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the laboratory session of day three, we will discover how we can implement a Logistic regression class from scratch but in this session would be examining the ready-made version from the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAADCCAYAAACG7BS5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3bbab7d5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "Y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "logreg.fit(X, Y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code gives a taste of how the sklearn library can perform various forms of regression and classification tasks as well as some advanced visualisation techniques that demonstrates how the algorithm performs discrimination among classes using a plot.  We can see here that the discrimination is still not perfect especially as two classes pretty much intermingle within each other. In the next section, we examine other important data cleaning operations.  This includes how we can produce some of these plots above, dealing with inconsistent data and most importantly the importance of dividing up the dataset into various samples for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Data cleaning operations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# In[3]:\n",
    "housing.info()\n",
    "\n",
    "# In[5]:\n",
    "housing.describe()\n",
    "\n",
    "# In[6]:\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices=np.random.permutation(len(data))\n",
    "    test_set_size=int(len(data) * test_ratio)\n",
    "    test_indices=shuffled_indices[:test_set_size]\n",
    "    train_indices=shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices],data.iloc[test_indices]\n",
    "\n",
    "# In[7]:\n",
    "train_set,test_set=split_train_test(housing,0.2)\n",
    "print(len(train_set), \"train +\", len(test_set),\"test\")\n",
    "\n",
    "# In[8]:\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "\n",
    "# In[9]:\n",
    "housing[\"median_income\"].hist()\n",
    "\n",
    "# In[10]:\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n",
    "\n",
    "# In[11]:\n",
    "corr_matrix=housing.corr()\n",
    "\n",
    "# In[12]:\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)\n",
    "\n",
    "# In[13]:\n",
    "from pandas import scatter_matrix\n",
    "attributes = ['median_house_value','median_income', 'total_rooms','housing_median_age']\n",
    "scatter_matrix(housing[attributes],figsize=(12,8))\n",
    "\n",
    "# In[14]:\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"ocean_proximity\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "\n",
    "# In[15]:\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "# In[16]:\n",
    "sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\n",
    "sample_incomplete_rows\n",
    "\n",
    "# ## Dealing with incomplete data options\n",
    "# 1. Remove all the rows with blanks\n",
    "# 2. Remove the column entirely\n",
    "# 3. Fill with median value\n",
    "\n",
    "# In[17]:\n",
    "sample_incomplete_rows.drop('total_bedrooms', axis=1)\n",
    "\n",
    "# In[18]:\n",
    "median = housing[\"total_bedrooms\"].median() #calculate the median value\n",
    "sample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # option 3\n",
    "sample_incomplete_rows\n",
    "\n",
    "# In[19]:\n",
    "housing_num=housing.drop('ocean_proximity', axis=1)\n",
    "\n",
    "# In[20]:\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer=Imputer(strategy='median')\n",
    "imputer.fit(housing_num)\n",
    "\n",
    "# In[21]:\n",
    "imputer.statistics_\n",
    "\n",
    "# In[22]:\n",
    "housing_num.median().values\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "X=imputer.transform(housing_num)\n",
    "housing_tr=pd.DataFrame(X,columns=housing_num.columns,index=list(housing.index.values))\n",
    "\n",
    "housing_tr.loc[sample_incomplete_rows.index.values]\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering and Cross Validation (inc)\n",
    "\n",
    "When we compare results of various classification algorithms and visualise our data we are in a better position to select the features that are more likely to yield better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the Machine Learning Pipeline Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Implementing NER using Logistic Regression\n",
    "\n",
    "The challenge for this session introduces a Spanish Corpus of text.  This is simply a large collection of text in Spanish.  In this laboratory session challenge we are going to do some Natural Language Processing NLP.  In particular our goal today's laboratory session challenge is to perform Named Entity Recognition (NER) on the body of text.  That is, for each word found in the text, we wish to identify if the word is a Named Entity.  A Named Entity is simply A noun that can be a person's name or a named location. The classes we wish to identify in this exercise include the following:\n",
    "\n",
    "\n",
    "So how can we use Logistic regression for this task?   The first thing to do is to tokenize the body of text into individual words and sentences; and because we are performing a supervised learning task, we need to perform a manual categorisation of each task into the classes given above. \n",
    "\n",
    "We then aggregate our findings into a table.  This preprocessing table has already been provided for us in a regular csv file.  In addition, some additional features were added into the mix to see if the logistic regression can perform better with the extra features added.  The features given in the dataset is described as follows:\n",
    "\n",
    "\n",
    "Let us examine the dataset based on the tools we have at our disposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>+1:postag</th>\n",
       "      <th>+1:postag[:2]</th>\n",
       "      <th>+1:word.istitle()</th>\n",
       "      <th>+1:word.isupper()</th>\n",
       "      <th>+1:word.lower()</th>\n",
       "      <th>-1:postag</th>\n",
       "      <th>-1:postag[:2]</th>\n",
       "      <th>-1:word.istitle()</th>\n",
       "      <th>-1:word.isupper()</th>\n",
       "      <th>...</th>\n",
       "      <th>bias</th>\n",
       "      <th>nerlabel</th>\n",
       "      <th>postag</th>\n",
       "      <th>postag[:2]</th>\n",
       "      <th>word.isdigit()</th>\n",
       "      <th>word.istitle()</th>\n",
       "      <th>word.isupper()</th>\n",
       "      <th>word.lower()</th>\n",
       "      <th>word[-2:]</th>\n",
       "      <th>word[-3:]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Fpa</td>\n",
       "      <td>Fp</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>NP</td>\n",
       "      <td>NP</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>melbourne</td>\n",
       "      <td>ne</td>\n",
       "      <td>rne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NP</td>\n",
       "      <td>NP</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>australia</td>\n",
       "      <td>NP</td>\n",
       "      <td>NP</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>O</td>\n",
       "      <td>Fpa</td>\n",
       "      <td>Fp</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "      <td>(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Fpt</td>\n",
       "      <td>Fp</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>)</td>\n",
       "      <td>Fpa</td>\n",
       "      <td>Fp</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>NP</td>\n",
       "      <td>NP</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>australia</td>\n",
       "      <td>ia</td>\n",
       "      <td>lia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Fc</td>\n",
       "      <td>Fc</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>,</td>\n",
       "      <td>NP</td>\n",
       "      <td>NP</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>O</td>\n",
       "      <td>Fpt</td>\n",
       "      <td>Fp</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Z</td>\n",
       "      <td>Z</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "      <td>Fpt</td>\n",
       "      <td>Fp</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>O</td>\n",
       "      <td>Fc</td>\n",
       "      <td>Fc</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 +1:postag +1:postag[:2] +1:word.istitle() +1:word.isupper()  \\\n",
       "0           0       Fpa            Fp             False             False   \n",
       "1           1        NP            NP              True             False   \n",
       "2           2       Fpt            Fp             False             False   \n",
       "3           3        Fc            Fc             False             False   \n",
       "4           4         Z             Z             False             False   \n",
       "\n",
       "  +1:word.lower() -1:postag -1:postag[:2] -1:word.istitle() -1:word.isupper()  \\\n",
       "0               (         -             -                 -                 -   \n",
       "1       australia        NP            NP              True             False   \n",
       "2               )       Fpa            Fp             False             False   \n",
       "3               ,        NP            NP              True             False   \n",
       "4              25       Fpt            Fp             False             False   \n",
       "\n",
       "     ...     bias  nerlabel  postag  postag[:2] word.isdigit() word.istitle()  \\\n",
       "0    ...      1.0     B-LOC      NP          NP          False           True   \n",
       "1    ...      1.0         O     Fpa          Fp          False          False   \n",
       "2    ...      1.0     B-LOC      NP          NP          False           True   \n",
       "3    ...      1.0         O     Fpt          Fp          False          False   \n",
       "4    ...      1.0         O      Fc          Fc          False          False   \n",
       "\n",
       "  word.isupper()  word.lower()  word[-2:]  word[-3:]  \n",
       "0          False     melbourne         ne        rne  \n",
       "1          False             (          (          (  \n",
       "2          False     australia         ia        lia  \n",
       "3          False             )          )          )  \n",
       "4          False             ,          ,          ,  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "nerds=pd.read_csv('tsents.csv',encoding='utf-8')\n",
    "nerds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The first thing to observe is that the dataset is all text with a few true and false data and only one numeric data column.  However, we recall that logistic regression only takes numeric inputs.  Thankfully, we have our one-hot-encoding tool and Machine learning pipeline at our disposal to perform this task.  We will therefore perform prepocessing of this dataset using the Machine learning pipeline including the one-hot encoding and then test our dataset using the Logistic Regression class of the Scikit-Learn Library we examined earlier.  Note that we only need to one-hot-encode the inputs because the Logistic Regression class of the Scikit-learn library takes care of encoding the labels internally for us.\n",
    "\n",
    "For this exercise we are only interested in using the column (feature) word.lower as the input data and nerlabel as the output label.  We will put them into a dataset using the DataFrame command as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>nerlabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>melbourne</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>australia</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>)</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        data nerlabel\n",
       "0  melbourne    B-LOC\n",
       "1          (        O\n",
       "2  australia    B-LOC\n",
       "3          )        O\n",
       "4          ,        O"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=pd.DataFrame({'data':nerds['word.lower()'],'nerlabel':nerds['nerlabel']})\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "de               17806\n",
       ",                14716\n",
       "la               10471\n",
       "el                8271\n",
       "que               7528\n",
       ".                 7263\n",
       "en                6935\n",
       "\"                 5691\n",
       "y                 5367\n",
       "a                 4443\n",
       "los               4019\n",
       "del               3772\n",
       "por               2653\n",
       "las               2544\n",
       "se                2537\n",
       ")                 2099\n",
       "(                 2094\n",
       "con               2080\n",
       "un                2047\n",
       "para              1799\n",
       "una               1684\n",
       "-                 1582\n",
       "su                1459\n",
       "al                1402\n",
       "no                1156\n",
       "efe                997\n",
       "ha                 964\n",
       "hoy                798\n",
       "es                 763\n",
       "como               756\n",
       "                 ...  \n",
       "espaolista          1\n",
       "propugn             1\n",
       "departamental        1\n",
       "modificar           1\n",
       "levanta              1\n",
       "visualizar           1\n",
       "peces-barba          1\n",
       "104'57'              1\n",
       "rectorado            1\n",
       "causacin            1\n",
       "neozelandes          1\n",
       "regido               1\n",
       "expresara            1\n",
       "maran                1\n",
       "dennis               1\n",
       "22.00                1\n",
       "362.56               1\n",
       "nervios              1\n",
       "emblemticos         1\n",
       "definiciones         1\n",
       "kee                  1\n",
       "herman              1\n",
       "verle                1\n",
       "sumergida            1\n",
       "db3885               1\n",
       "fifa                 1\n",
       "calor                1\n",
       "ligar                1\n",
       "terrassa             1\n",
       "ee.uu                1\n",
       "Name: data, Length: 23710, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then perform the following tasks\n",
    "1. Perform our one hot encoding on the whole dataset\n",
    "2. Split the data into training and test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"ocean_proximity\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### SkLearn One Hot Encoder\n",
    "# In[30]:\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder=OneHotEncoder()\n",
    "housing_cat_1hot=encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
    "housing_cat_1hot\n",
    "\n",
    "# In[31]:\n",
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Session Challenge Word Vectors vs Bag of Words<a id=\"sec-2-4\" name=\"sec-2-4\"></a>\n",
    "The above implementation of using one-hot encoding is known as the bag of words model.  There exists another model in which rather than treating words as independent entities, in the bag we try to establish relationships between them.  A particular model or class of models is called the Vord2Vector model.\n",
    "\n",
    "This model performs a form of encoding called a word embedding by mapping or learning a vector representation of words in context to other words.  This can be learned through various means see (Mikolov et. al, 2013).  In the next class we will see how Logistic regression can be use to discover these word vectors. One way to think of this model is using a co-occurrence matrix decomposition. Appendix 1 explains this co-occurrence model in detail.\n",
    "\n",
    "For this session challenge we will use a word2vec library to discover the word embeddings for our corpus. The results of this word2vec transformation will now be fed into our initial model instead of the original one-hot encoded vector and accuracies shall be compared using cross validation we considered in this laboratory session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.2.0-cp36-cp36m-manylinux1_x86_64.whl (15.9MB)\n",
      "\u001b[K    100% |################################| 15.9MB 79kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open>=1.2.1 (from gensim)\n",
      "  Downloading smart_open-1.5.5.tar.gz\n",
      "Requirement already satisfied: six>=1.5.0 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: boto>=2.32 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Collecting bz2file (from smart-open>=1.2.1->gensim)\n",
      "  Downloading bz2file-0.98.tar.gz\n",
      "Requirement already satisfied: requests in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: boto3 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: botocore<1.9.0,>=1.8.0 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from botocore<1.9.0,>=1.8.0->boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: docutils>=0.10 in /home/nbcommon/anaconda3_501/lib/python3.6/site-packages (from botocore<1.9.0,>=1.8.0->boto3->smart-open>=1.2.1->gensim)\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/ed/64/56/a922ace26f5d32090849ec8d89192b2b9ff0d280a5be0a4b7b\n",
      "  Running setup.py bdist_wheel for bz2file ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/31/9c/20/996d65ca104cbca940b1b053299b68459391c01c774d073126\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: bz2file, smart-open, gensim\n",
      "Successfully installed bz2file-0.98 gensim-3.2.0 smart-open-1.5.5\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Appendix 1: Word2Vec Co-occurrence Matrix Decomposition with SVD Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For the co-occurrence model we count the number of times a certain number of words occur together take for example a vocuabulary of words and a count of the times two words co-occur together in the illustration below:\n",
    "![Co-occurrence Matrix](https://www.dropbox.com/s/pcl4c8lf5as56tl/svd1.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Corpus:\n",
    "- I like deep learning.\n",
    "- I like NLP.\n",
    "- I enjoy flying.\n",
    "\n",
    "The word vectors can therefore be determined using Singular Value Decomposition (SVD) as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAFqCAYAAAA5ngEFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAGDNJREFUeJzt3X+w3XV95/HnCwKISNXdxA6SIKhB\niJQuehfZuqNQWAeoDY61AltGcSkZu6VOrWsH1g661M5QW+vWWaxmLbUyIiI7pcFBsaNQrCsOYVGU\nsHRTRIiwEvm1CCEQee8f55vmcLk39yS555zPPff5mDlzzvfz/Xy/930+c2/mlc/ne74nVYUkSVLL\n9hp3AZIkSXMxsEiSpOYZWCRJUvMMLJIkqXkGFkmS1DwDiyRJap6BRRMtyaVJHkjy/Vn2J8nHk2xM\ncluS14y6RknS3AwsmnSfAU7eyf5TgJXdYw3wFyOoSZK0iwwsmmhVdSPw0E66nAZ8tnpuAl6U5KDR\nVCdJGpSBRYvdwcC9fdubujZJUkOWjLsAacwyQ9uM31eRZA29ZSMOOOCA1x5xxBHDrEuSJs4tt9zy\nk6patjvHGli02G0CVvRtLwfum6ljVa0F1gJMTU3V+vXrh1+dJE2QJD/c3WNdEtJitw54R/dpoeOA\nR6vq/nEXJUl6NmdYNNGSfB44HliaZBPwQWAfgKr6JHAtcCqwEXgCeNd4KpUk7YyBRROtqs6cY38B\nvz2iciRJu8klIUmS1DwDiyRJap6BRZIkNc/AIkmSmmdgkSRJzTOwSJKk5hlYJElS8wwskiSpeQYW\nSZLUPAOLJElqnoFFkiQ1z8AiSZKaZ2CRJEnNM7BIkqTmGVgkSVLzDCySJKl5BhZJktQ8A4skSWqe\ngUWSJDXPwCJJkppnYJEkSc0zsEiSpOYZWCRJUvMMLJIkqXkGFkmS1DwDiyRJap6BRZIkNc/AIkmS\nmmdgkSRJzTOwSJKk5hlYJElS8wwskiSpeQYWSZLUPAOLJElqnoFFkiQ1z8CiiZfk5CR3JtmY5PwZ\n9h+S5Poktya5Lcmp46hTkjQ7A4smWpK9gUuAU4BVwJlJVk3r9gfAlVV1DHAG8InRVilJmouBRZPu\nWGBjVd1VVU8BVwCnTetTwM91r18I3DfC+iRJAzCwaNIdDNzbt72pa+v3IeCsJJuAa4HfmelESdYk\nWZ9k/ebNm4dRqyRpFgYWTbrM0FbTts8EPlNVy4FTgcuSPOdvo6rWVtVUVU0tW7ZsCKVKkmZjYNGk\n2wSs6NteznOXfM4BrgSoqm8BzwOWjqQ6SdJADCyadDcDK5MclmRfehfVrpvW5x7gRIAkR9ILLK75\nSFJDDCyaaFW1DTgPuA64g96ngW5PclGS1V239wHnJvku8Hng7KqavmwkSRqjJeMuQBq2qrqW3sW0\n/W0X9r3eALx+1HVJkgbnDIskSWqegUWSJDXPwCJJkppnYJEkSc0zsEiSpOYZWCRJUvMMLJIkqXkG\nFkmS1DwDiyRJap6BRZIkNc/AIkmSmmdgkSRJzTOwSJKk5hlYJElS8wwskiSpeQYWSZLUPAOLJElq\nnoFFkiQ1z8AiSZKaZ2CRJEnNM7BIkqTmGVgkSVLzDCySJKl5BhZJktS8OQNLkuqej0myqXv96SS3\nDbs4SZIkgCWDdqyqW4HlQ6xFkiRpRgMvCSX5t0menKH9Q0keS3J4kiOS/CjJ493j3fNbriRJWoz2\n6BqWJBcD7wV+oar+EfgK8OGqOgA4EfjzPS9RkiQtdgMvCc3gcOAw4Iiq+lHXdgjwZ0n+bPv5kxxU\nVffvSZGSJGlx25MZlkeAfYBfntb+0qrav3vsbViRJEl7ak8CywPAW4BPJ1ndtd0DXLa9Q5LT9+D8\nkiRJwB5ew1JVXwHeCXwxyQnAm4Cjk2xJshW4cB5qlCRJi9yc17BUVbrnfwCe173+zb79VwBX9B1y\nyDzXKEmSFjnvdCtJkppnYNHES3JykjuTbExy/ix93p5kQ5Lbk1w+6holSTu3Jx9rlpqXZG/gEuDf\nAZuAm5Osq6oNfX1WAhcAr6+qh5O8ZDzVSpJm4wyLJt2xwMaququqnqJ3vdVp0/qcC1xSVQ8DVNUD\nI65RkjQHA4sm3cHAvX3bm7q2focDhyf5ZpKbkpw8suokSQNxSUiTLjO01bTtJcBK4Hh6X/D5jSRH\nVdUjzzpRsgZYA3DIIX4YTpJGyRkWTbpNwIq+7eXAfTP0+duqerqqfgDcSS/APEtVra2qqaqaWrZs\n2dAKliQ9l4FFk+5mYGWSw5LsC5wBrJvW52rgBIAkS+ktEd010iolSTtlYNFEq6ptwHnAdcAdwJVV\ndXuSi/q+UuI64MEkG4DrgfdX1YPjqViSNJNUTV/OlzSXqampWr9+/bjLkKQFJcktVTW1O8c6wyJJ\nkppnYJEkSc0zsEiSpOYZWCRJUvMMLJIkqXkGFkmS1DwDiyRJap6BRZIkNc/AIkmSmmdgkSRJzTOw\nSJKk5hlYJElS8wwskiSpeQYWSZLUPAOLJElqnoFFkiQ1z8AiSZKaZ2CRJEnNM7BIkqTmGVgkSVLz\nDCySJKl5BhZJktQ8A4skSWqegUWSJDXPwCJJkppnYJEkSc0zsEiSpOYZWCRJUvMMLJIkqXkGFkmS\n1DwDiyRJap6BRZIkNc/AIkmSmmdgkSRJzTOwSJKk5hlYNPGSnJzkziQbk5y/k35vS1JJpkZZnyRp\nbgYWTbQkewOXAKcAq4Azk6yaod+BwHuAb4+2QknSIAwsmnTHAhur6q6qegq4Ajhthn5/CHwEeHKU\nxUmSBmNg0aQ7GLi3b3tT1/bPkhwDrKiqL+3sREnWJFmfZP3mzZvnv1JJ0qwMLJp0maGt/nlnshfw\nMeB9c52oqtZW1VRVTS1btmweS5QkzcXAokm3CVjRt70cuK9v+0DgKOCGJHcDxwHrvPBWktpiYNGk\nuxlYmeSwJPsCZwDrtu+sqkeramlVHVpVhwI3Aaurav14ypUkzcTAoolWVduA84DrgDuAK6vq9iQX\nJVk93uokSYNaMu4CpGGrqmuBa6e1XThL3+NHUZMkadc4wyJJkppnYJEkSc0zsEiSpOYZWCRJUvMM\nLJIkqXkGFkmS1DwDiyRJap6BRZIkNc/AIkmSmmdgkSRJzTOwSJKk5hlYJElS8wwskiSpeQYWSZLU\nPAOLniPJ55J8atx1SJK03ZJxF6D2VNVvjLsGSZL6OcOySCT5RJKfJtmSZEOSfZJUkn/o2h5L8uqu\n7w1Jrulen97t25LkviSHJjkhyRN95z4pyePjem+SpMlnYFkEkvwK8FZgWVXtDzwD/Ldu99e7tg3A\nf53h8M8AH+j6/B/gb6rqemBrktO7Pv8FuHaIb0GStMgZWBaHdwLLgIeSbAEOB17V7ftg9/wtYEX/\nQUlWAPtU1ce7pg/1HXc5cH6SfYBjgQuGVr0kadHzGpbFIcC3q+qXntWYVFVVt7mNXft9+M/AZnoh\n5v9W1cb5KFSSpJk4w7I4fBaYSrIKIMnLk/zSHMdQVfcCTyf57a7pg8D/7vY9CtwBnA+sHUrVkiR1\nDCyLQFVdA3wCuKVbEvoecORch3XPZwMXd8e9it61MNttXyq6eP6qlSTpuQwsi0RV/W5V7d89Dqiq\nv6yq9O3/T1X1ym7zxfSWe6iqL1TVgd1xB1XV3X2nfQvwzap6emRvRJK0KHkNi54lyY3AK4Bfm6Pf\nffSCzS+Ooi5J0uJmYNGzVNUbBuz30mHXIknSdi4JSZKk5hlYJElS8wwskiSpeQYWSZLUPAOLJElq\nnoFFkiQ1z8AiSZKaZ2CRJEnNM7BIkqTmGVg08ZKcnOTOJBuTnD/D/t9LsiHJbUm+luRl46hTkjQ7\nA4smWpK9gUuAU4BVwJlJVk3rdiswVVVHA1cBHxltlZKkuRhYNOmOBTZW1V1V9RRwBXBaf4equr6q\nnug2bwKWj7hGSdIcDCyadAcD9/Ztb+raZnMO8OWhViRJ2mV+W7MmXWZoqxk7JmcBU8AbZ9m/BlgD\ncMghh8xXfZKkATjDokm3CVjRt70cuG96pyQnAR8AVlfV1plOVFVrq2qqqqaWLVs2lGIlSTMzsGjS\n3QysTHJYkn2BM4B1/R2SHAN8il5YeWAMNUqS5mBg0USrqm3AecB1wB3AlVV1e5KLkqzuuv0J8ALg\ni0m+k2TdLKeTJI2J17Bo4lXVtcC109ou7Ht90siLkiTtEmdYJElS8wwskiSpeQYWSZLUPAPLIpTk\nge6TMZIkLQhedLsIVdVLxl2DJEm7whkWSZLUPAOLJElqnoFFkiQ1z8AiSZKaZ2CRJEnNM7BIkqTm\nGVgkSVLzDCySJKl5BhZJktQ8A4skSWqegUWSJDXPwCJJkppnYJEkSc0zsEiSpOYZWCRJUvMMLJIk\nqXkGFkmS1DwDiyRJap6BRbstSY3gZ3w4yZeH/XMkSW1bMu4CpCT7VNXTM+2rqj8YdT2SpPY4w6J5\nkeRLSR5PsiXJDX3t93ftTya5rK+9kvx9kp8Cv5lkW5IbkjzR9T2l6/fpJLd1rzcm+U6S/5fk6SR/\n2rXvneT73XE/TvLA9n2SpMlgYNEeS3I+8ArgBd3jyCTndbtfX1UHAAcBv57klX2H/q+qekFV/UW3\nvbmqng9cDXxslh/3L4EXA28F3tO1/THwEuAA4CRg6fy8M0lSKwwsmg+/BqwEngB+Si9QvLbbd2mS\nLcB9wH7AG/uO+/1p57m4e/4qsGyWn7Wuqn5WVdcA+3RtJwLXdO3fA+7fkzcjSWqP17BoPgS4oqrO\nelZj8rvAMcDyqnowySPAgdv3z3DdymPd89PA3rP8rC3zU7IkaSFxhkXz4SrgLUl+HiDJa5OsojdL\nsqULK6cALxzSz/868ObuWpZX01t+kiRNEGdYtMeq6uIk/xr4YRLozZCsprfEs6ZbEvoJ8OiQSvh9\n4GR6S1IPAQ8Cm4f0syRJY5Cqod9KQxq6JD9fVT/uLuq9A3hNdz3LUExNTdX69euHdXpJmkhJbqmq\nqd051hkWTYo7k+xHb5nzr4cZViRJo2dg0USoqheNuwZJ0vB40a0kSWqegUUTL8nJSe7s7pR7/gz7\n90vyhW7/t5McOvoqJUk7Y2DRREuyN3AJcAqwCjiz+8h1v3OAh6vqlfTusPvHo61SkjQXA4sm3bHA\nxqq6q6qeAq4ATpvW5zTgr7vXVwEnpvt8tiSpDV50q0l3MHBv3/Ym4HWz9amqbUkepfedRT/p75Rk\nDbCm29ya5PtDqXjhWcq0sVrEHIsdHIsdHIsdXrW7BxpYNOlmmimZfvOhQfpQVWuBtQBJ1u/uvQQm\njWOxg2Oxg2Oxg2OxQ5LdvoGVS0KadJuAFX3by+l9EeOMfZIsofcVAg+NpDpJ0kAMLJp0NwMrkxyW\nZF/gDGDdtD7rgHd2r98GfL28BbQkNcUlIU207pqU84Dr6H0D9KVVdXuSi4D1VbUO+EvgsiQb6c2s\nnDHAqdcOreiFx7HYwbHYwbHYwbHYYbfHwu8SkiRJzXNJSJIkNc/AIkmSmmdgkXbC2/rvMMBY/F6S\nDUluS/K1JC8bR52jMNdY9PV7W5JKMrEfaR1kLJK8vfvduD3J5aOucVQG+Bs5JMn1SW7t/k5OHUed\nw5bk0iQPzHavqvR8vBun25K8ZqATV5UPHz5meNC7SPefgJcD+wLfBVZN6/MfgU92r88AvjDuusc4\nFicAz+9e/9ZiHouu34HAjcBNwNS46x7j78VK4Fbgxd32S8Zd9xjHYi3wW93rVcDd4657SGPxBuA1\nwPdn2X8q8GV698A6Dvj2IOd1hkWanbf132HOsaiq66vqiW7zJnr3vJlEg/xeAPwh8BHgyVEWN2KD\njMW5wCVV9TBAVT0w4hpHZZCxKODnutcv5Ln3hJoIVXUjO7+X1WnAZ6vnJuBFSQ6a67wGFml2M93W\n/+DZ+lTVNmD7bf0nzSBj0e8cev+DmkRzjkWSY4AVVfWlURY2BoP8XhwOHJ7km0luSnLyyKobrUHG\n4kPAWUk2AdcCvzOa0pqzq/+eAN6HRdqZebut/wQY+H0mOQuYAt441IrGZ6djkWQvet/6ffaoChqj\nQX4vltBbFjqe3qzbN5IcVVWPDLm2URtkLM4EPlNVH03yb+jd/+moqnpm+OU1Zbf+3XSGRZqdt/Xf\nYZCxIMlJwAeA1VW1dUS1jdpcY3EgcBRwQ5K76a3Rr5vQC28H/Rv526p6uqp+ANxJL8BMmkHG4hzg\nSoCq+hbwPHpfjLjYDPTvyXQGFml23tZ/hznHolsG+RS9sDKp1ynAHGNRVY9W1dKqOrSqDqV3Pc/q\nqtrtL31r2CB/I1fTuyCbJEvpLRHdNdIqR2OQsbgHOBEgyZH0AsvmkVbZhnXAO7pPCx0HPFpV9891\nkEtC0ixqeLf1X3AGHIs/AV4AfLG77vieqlo9tqKHZMCxWBQGHIvrgDcl2QD8DHh/VT04vqqHY8Cx\neB/w35O8l94SyNmT+B+cJJ+ntwS4tLte54PAPgBV9Ul61++cCmwEngDeNdB5J3CsJEnShHFJSJIk\nNc/AIkmSmmdgkSRJzTOwSJKk5hlYJElS8wwskiSpeQYWSZLUPAOLJElqnoFFkiQ1z8AiSZKaZ2CR\nJEnNM7BoXiS5KsnWJD9Lctscff8+yftHVZskaeHzyw81L5JsBU4C3gkcW1VHj7kkSdIEcYZFeyzJ\n7cC+wN8BS7u2g5I8nWT/bvvgJNuS7J9kY5I/7dq3JbkhyRNJnkxyStd+RJIHu/YNXb/Dx/QWJUlj\nZmDRHquqVwM/A44GftK13Q/cDVzYdbsY+E5VbZnhFJur6vnA1cDHurYrgZu79s8Bew/tDUiSmmdg\n0TB9FDi7e/0W4MOz9Lu4e/4qsKx7vRL4I4Cq+iPAtUtJWsQMLBqaqvok8KIk76F3vdTVs3R9rHt+\nGmdSJEkzMLBo2P6O3jLPul08biNwAUCS84HMc12SpAXEwKJh+wC937MLdvG404HjkjwBvB14Brh/\nnmuTJC0QfqxZQ9V9GuitVfXyXTzuQOCpqtqa5Fzg41W1/1CKlCQ1b8m4C9DkSvJd4Ejgzbtx+OuA\ndUlCb3bl3fNZmyRpYXGGRZIkNc9rWCRJUvMMLJIkqXkGFkmS1DwDiyRJap6BRZIkNc/AIkmSmmdg\nkSRJzTOwSJKk5hlYJkiSG5JcM+46JEmabwYWSZLUPAPLApfkq0meSvIgcFjXdkKSzUkeT/JoklO6\n9iOS/KhrfzzJu7v2G5LcleSh7lyfHeNbkiTpOQwsC1iS3wDeABwEHN09A/wP4MyqOgB4L/C5rv0r\nwIe79hOBP+873UHAK4CjgH+f5JjhvwNJkgbjlx8uYEn+BvgXVfXGbvsW4CHgJODJvq57VdV+SZ4B\ntva17wssBz5P73dh+3n+Cbiyqi4YwduQJGlOS8ZdgPbY9MS5F/BMVe0/S/+XVtXD/Q1JZjrPM/NT\nniRJe84loYXti8Drkrw4yUHALwBPAI8n+ShAen69638PcNn2g5Oc3neu1yV5YZJXAi8DrhrJO5Ak\naQAGlgWsqi4HvgH8GPgecHe361eAdyTZQm9p6N1d+5uAo5NsSbIVuLDvdHcDPwA2AJdX1a1DfwOS\nJA3Ia1hEkhuAx6rqV8ddiyRJM3GGRZIkNc8ZlgUgSQG3VNVUt30NcGBVHT/b7Eh3zJNAgM3Av6qq\nB0dbuSRJ88MZloXjF5McvisHVNX+VfU8YBtw6XDKkiRp+AwsC8fX2P3Q8T/p3RROkqQFyfuwLBzn\nAD9MsmJXDkqyH/DLwPVDqUqSpBFwhmWBqKofAd8E/mrQY7qPNT8CPACcO6TSJEkaOmdYFpZ3Af8I\n3DhI553c7VaSpAXFGZYFpKruAm6h94WHkiQtGgaWhec/AHtPa3tzkm3bH+MoSpKkYfI+LJIkqXnO\nsEiSpOYZWCRJUvMMLJIkqXkGFkmS1DwDiyRJap6BRZIkNc/AIkmSmmdgkSRJzTOwSJKk5hlYJElS\n8wwskiSpeQYWSZLUPAOLJElqnoFFkiQ1z8AiSZKaZ2CRJEnNM7BIkqTmGVgkSVLzDCySJKl5BhZJ\nktQ8A4skSWqegUWSJDXPwCJJkppnYJEkSc0zsEiSpOYZWCRJUvMMLJIkqXkGFkmS1DwDiyRJap6B\nRZIkNc/AIkmSmmdgkSRJzTOwSJKk5hlYJElS8wwskiSpeQYWSZLUPAOLJElqnoFFkiQ1z8AiSZKa\nZ2CRJEnNM7BIkqTmGVgkSVLzDCySJKl5BhZJktQ8A4skSWqegUWSJDXPwCJJkppnYJEkSc0zsEiS\npOYZWCRJUvMMLJIkqXljDyxJatw1SJKkto09sEiSJM3FwCJJkppnYJEkSc0zsEiSpOYZWCRJUvMM\nLJIkqXkGFkmS1LxUeRsUSZLUNmdYJElS8wwskiSpeQYWSZLUPAOLJElqnoFFkiQ1z8AiSZKaZ2CR\nJEnNM7BIkqTm/X+TwPVu5jdttAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe30f6c89b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "la=np.linalg\n",
    "words=['I','like','enjoy','deep','learning','NLP','flying','.']\n",
    "x=np.array([[0,2,1,0,0,0,0,0],\n",
    "            [2,0,0,1,0,1,0,0],\n",
    "            [1,0,0,0,0,0,1,0],\n",
    "            [0,1,0,0,1,0,0,0],\n",
    "            [0,0,0,1,0,0,0,1],\n",
    "            [0,1,0,0,0,0,0,1],\n",
    "            [0,0,1,0,0,0,0,1],\n",
    "            [0,0,0,0,1,1,1,0]\n",
    "           ])\n",
    "U,s,Vh=la.svd(x,full_matrices=False)\n",
    "for i in range(len(words)):\n",
    "    plt.text(U[i,0],U[i,1],words[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -5.24124930e-01,  -5.72859145e-01,   9.54463014e-02,\n",
       "          3.83228493e-01,  -1.76963375e-01,  -1.76092183e-01,\n",
       "         -4.19185600e-01,  -5.57702732e-02],\n",
       "       [ -5.94438071e-01,   6.30120664e-01,  -1.70207779e-01,\n",
       "          3.10038363e-01,   1.84062339e-01,  -2.34777849e-01,\n",
       "          1.29535474e-01,   1.36813128e-01],\n",
       "       [ -2.56274005e-01,   2.74017533e-01,   1.59810848e-01,\n",
       "         -5.55111512e-16,  -5.78984617e-01,   6.36550929e-01,\n",
       "         -6.10622664e-16,  -3.05414877e-01],\n",
       "       [ -2.85637408e-01,  -2.47912130e-01,   3.54610324e-01,\n",
       "         -7.31901294e-02,   4.45784489e-01,   8.36141432e-02,\n",
       "          5.48721075e-01,  -4.68012411e-01],\n",
       "       [ -1.93139313e-01,   3.38495048e-02,  -5.00790405e-01,\n",
       "         -4.28462480e-01,   3.47110226e-01,   1.55483227e-01,\n",
       "         -4.68663749e-01,  -4.03576557e-01],\n",
       "       [ -3.05134684e-01,  -2.93988990e-01,  -2.23433593e-01,\n",
       "         -1.91614246e-01,   1.27460940e-01,   4.91219408e-01,\n",
       "          2.09592800e-01,   6.57535375e-01],\n",
       "       [ -1.82489837e-01,  -1.61027767e-01,  -3.97842428e-01,\n",
       "         -3.83228493e-01,  -5.12923221e-01,  -4.27574417e-01,\n",
       "          4.19185600e-01,  -1.18313828e-01],\n",
       "       [ -2.46898426e-01,   1.57254762e-01,   5.92991677e-01,\n",
       "         -6.20076727e-01,  -3.21868120e-02,  -2.31065080e-01,\n",
       "         -2.59070949e-01,   2.37976916e-01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
